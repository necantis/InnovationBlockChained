---
title: Blocked and Chained? An approach to enable open innovation through tokenization
author:
  - name: Author 01
    affiliation: University
    department: Department
    location: Location
    email: email@email.com
  - name: Author 02
    affiliation: University
    department: Department
    location: Location
    email: email@email.com
abstract: |
  The abstract goes here.

bibliography: ECIS2019.bib
output: rticles::ieee_article
---

Introduction
=============
<!-- no \IEEEPARstart -->
This paper presents the preliminary results of an ongoing study that aims at designing new technological solutions, such as blockchain, to allow firms to perform open innovation by means of decentralized idea challenges. Open innovation is a paradigm that assumes that firms can and should use external ideas as well as internal ideas, and internal and external paths to market, as the firms look to advance their technology  [@chesbrough_open_2006,@vanhaverbeke_classification_2014]. 
Among the different technologies that have enabled open innovation, blockchain could actually be considered those that instantiate open innovation in an infrastructure for transactions, products, and services [@shrier_blockchain_2016]. Risen as the technology paradigm on which the bitcoin relied and spread [@nakamoto_bitcoin:_2008], blockchain has been receiving increasing attention as the basis for applications beyond cryptocurrencies [@tapscott_this_2017]. This is due to the individual relevance of different technologies making it up, such as smart contracts, encryption, and distributed Ledger [@halaburda_blockchain_2018].  Taking these issues into account, a blockchain generally defined as a “distributed ledger of transactions” [@halaburda_blockchain_2018] is a continuously growing list of records, called blocks, which are linked and secured using cryptography. Each block typically contains a cryptographic hash of the previous block, a timestamp, and transaction data [@swan_blockchain_2015, @tapscott_blockchain_2016]. Blockchain allows tokenization, the process of substituting a sensitive data element with a non-sensitive equivalent, referred to as a token, that has no extrinsic or exploitable meaning or value.   
In this article we assess how to use blockchain to perform tokenization of content and to manage idea challenges in a decentralized fashion, in order to increase the number of innovative ideas, defined here as ideas that differ from each other and meet the requirements of the solution seeker. 
Thus, our research question is: how to use blockchain to manage idea challenges in a decentralized fashion while increasing the amount of innovative ideas?  
The rest of the paper proceeds as it follows. In section 2 we briefly review the existing literature and we identify the relevant concepts needed to answer our research question. Section 3 summarizes how we used design science to create an artefact under the shape of a method, whereas section 4 describes how we tested it by means of simulation. Section 5 concludes the paper by discussing its limitation and by suggesting further directions of investigation. 

Background and motivations
=============
In this section, we briefly review existing literature on management literature concerning different solutions to decentralize the innovation process.  
In the last decade, open innovation [@chesbrough_explicating_2014] has been more and more considered by private as well as public organizations [@viscusi_open_2015]  as a way to obtain competitive advantage through the exploitation of the opportunities and capabilities offered by digitalization [@tilson_research_2010, @yoo_tables_2013] and exploration of alternative solutions to research and development. Among the phenomena related to open innovation the tokenization of work and the adoption of collective contests for ideas searching and problem solving through crowdsourcing [@afuah_crowdsourcing_2012, @tucci_creating_2018, @boudreau_using_2013, @felin_firms_2017] received an increasing attention as a way to exploit collective intelligence for innovation as well as to improve performance of organizations [@malone_handbook_2015, @woolley_collective_2015 ,@riedl_teams_2017]. 
Taking these issues into account, web-based idea competitions such as Innocentive has already been proven successful at gathering a large set of solvers to address complex problems [@lakhani_open_2012]. @bullinger_community-based_2010-1 have reviewed 52 publications on ideas competition and identified 10 design elements for ideas competition, noticing the most common combination is: an (1) online idea competition (2) initiated by a firm (3) that sets a medium amount of specificity concerning the required tasks, which can range from (4) ideas, sketches, concepts, prototype, and working solution, for a (5) specific target group of (6) individuals that (7) competes and cooperates thanks to community functions in the online platform, for (8) a large amount of time, (9) in exchange of a mix between monetary and non-monetary rewards that are given by (10) a jury of experts. With respect to the process used to generate ideas, we refer to @faste_brainstorm_2013, who compared classic brainstorming with two new solutions: (a) chainstorming, where participants have to use ideas from a previous brainstorming to solve a new problem and (b) cheatstorming, where participants cannot use new ideas and can only use ideas from previous brainstorming. Since performance of cheatstorming seems to outperform the other options, the authors suggested that idea generation is less about idea and more about dealing with cultural influence exerted by unconventional ideas on the ideating team. Therefore, tokenization could be used to list the ideas submitted and rewards those that are winning as well as those that are reused, as long as a way to regroup similar ideas in a decentralized fashion.  

Methodology
=============
In this section we illustrate the chosen methodology to answer our research question. We position our study in the field of design science research [@hevner_design_2004] and we developed an artefact under the shape of a method, as defined by @march_design_1995. Accordingly, we have followed the guidelines of @gregor_positioning_2013 to create a set of design principles as part of a nascent design theory, which is at level 2 of the contribution types. Finally, we have selected simulation as an experimental way to test our assumptions ex-ante in a setting that allowed to control the variables.   Artificial  evaluation  can be seen as unreal  due to (a) unreal  users,  (b) unreal  systems,  and  (c)  unreal  problems  since it is not  held  by  the users and/or not real tasks, etc.[@sun_cross-evaluation:_2006]. Nonetheless, @pries-heje_strategies_2008[@venable_comprehensive_2012] recommend to use this type of artificial evaluation as an ex-ante assessment of the system, before testing it on larger scale with real users (naturalistic evaluations).
In what follows we present our artefact in the shape of a method composed of four phases, as shown in figure 1, and we illustrate how it works by a simple example. 

Table 1 illustrates the results of an idea competition concerning how to reduce energy consumption in private households. For sake of simplicity, we will assume to obtain four ideas. Each idea receives a unique identification number, which is associated with the unique ID of the user (Step 1). Then, by means of automatic or manual classification, the ideas are assigned to clusters showing the “stance” of each idea: for example, “behavioural” vs. “technical” or “engineering” oriented idea (Step 2).  For sake of discussion, we can also assume that a set of evaluators give a score between 1 and 9 for each idea according to three dimensions, which have different weights (D1 is 100, D2 is 10 and D3 is 1). Finally, the first idea of each cluster is selected, the selected players (065, 177 and 356) win a prize and are invited for phase 2.  
In the second phase, the players perform chainstorm and combine the ideas that passed phase 01, to produce new ideas. Table 2 shows the creation of two new ideas. In this step, the ideas are assessed by evaluators, who give a score between 1 and 9 for each idea according to three dimensions, which have different weights (D1 is 100, D2 is 10 and D3 is 1).  For sake of illustration, we can assume that the score of two ideas in one dimension is the sum of the scores of the two individual ideas in that dimension. That allows us to predict the score of idea 005 and idea 006, and choose idea 686, since it maximizes the satisfaction of the client.  
A standard idea challenge would have rewarded the best idea (in this case idea 003, with a score of 724). Nonetheless, that would require an expert assessment of all ideas, whereas our model allows assessing only the pooled ideas, which are the sum of possible combinations among ideas selected in phase 01. Hence, ideas to assess = {[1+(clusters/2)]*(cluster/2)]/2 
That leads to the first design guideline for a decentralized idea challenge, which is associated with a testable proposition.
**P1**: automatic clustering of tokenized ideas in a blockchain and random selection significantly reduce the number of ideas to assess but it decreases the quality of the outcome.
Moreover, one could expect the client to seek for a solution that scores 9 in every dimension, which leads to 999 as target in our example. Thus, it is possible to estimate the difference between the target and the chosen idea in phase 02, which is 999 – 687 = 312. Setting 312 as the new target, we could select the ideas from phase 01 that are below the target, in our case idea 002 and idea 004. Then, we could set up a cheatstorming competition that rewards all ideas below the target for contribution, and then select the best pool of ideas created on a previous context. In our case, the combination of idea 002 and idea 004 reaches 312. This idea would have not be retained in phase 02, but it now perfectly fits the narrow problem of the client. Moreover, the number of pooled ideas might be significant in this step, but it still below the number of ideas to assess by experts in a standard idea challenge.
That leads to the second design guideline for a decentralized idea challenge, which is associated with a testable proposition.
**P2**: collection of tokenized ideas in a blockchain done in a previous challenge increases the quality of the outcome but it increases the number of ideas to assess

We use R Statistical package  @r._core_team_r:_2013 to write an agent-based simulation, that is a computerized simulation of a number of decision-makers (agents), which interact through prescribed rules [@farmer_economy_2009]. Our code has four functions, corresponding to the following phases: 
•	phase 00 generates the agents and it assigns one or more ideas to each agent. It takes as initial parameters the target fixed by the client, the number of agents, the number of ideas, and the clusters. In the previous example, the target was 10^3, there were 3 agents that generated 4 ideas into 3 clusters in phase 01. Our simulation uses 10^9 as target, 10^3 agents, 10^4 ideas and 10 clusters.
•	phase 01 divides the ideas into clusters and retains one idea per cluster. We shall compare the case when the winner for each cluster is the first one (model A) or the one with the idea that has the greatest value (model B).
•	phase 02 selects and combines the retained idea into the best idea pool.  This is a problem of linear programming that tries to maximizes the sum of the ideas by defining that the coefficient of each idea is binary (either we select it or not) and by setting one constraint (the sum of the pooled idea cannot be greater than the target).
•	phase 03 assesses the average payoff of agents, by using the mean of the individual payoff, and the payoff of the client, by calculating the remainder between the target and the pooled ideas. For sake of clarity, we shall compare the situation when the idea challenge has 1 stage and one cluster (Model 0), when we add a second stage performing chainstorming (Model 1) and when we add a third stage performing cheatstorming.

Table 3 illustrates the outcome of the simulations. The first model (M0) allows to obtain a good idea selected among 10^4 ideas. That assures a good satisfaction for the client and a small cost in terms of prize for the only winner. Nonetheless, one can assume it would be fairly expensive to pay experts to evaluate the 10^4 ideas and it would be quite disappointing for the agents to have 1 chance out of 10^4 to win. Model 1a has 10 winners for the phase 01, that is one per cluster. One can assume that cluster definition could be done by an unsupervised algorithm for clustering based on textual description of the solution, so there will be no need to request expert evaluations. In the second stage, the ideas are pooled together and the outcome is below the one of the first model, and the agents to reward are 14 (10 in the first stage and 4 in the second stage). Nonetheless, the cost for idea assessment are significantly lower, since there are only 15 possible combinations to evaluate. This validates our first testable proposition (P1). Moreover, Model 1b appears evident why it would not be wise to select the best idea for every cluster. Indeed, it would take the same amount of assessment as the same model to obtain the same result (in the second phase, the agent with the best idea cannot find anyone that can improve it).

Finally, model 2 has been conceived to illustrate that with a large set of agents it would be theoretically possible to receive a number of ideas to assess that is greater than the initial number, although it is unlikely to occur in reality. This validates our second testable proposition (P2) under the condition that the number of retained ideas in the second idea challenge is not too large (although, one could choose to solve this problem by using idea clustering again).  

5	Discussions
====
In this section we assess our article by using the six guidelines for a design science paper of [@hevner_design_2004].  The first guideline states that design-science research must produce a viable artifact in the form of a construct, a model, a method, or an instantiation: our artefact is a model for a decentralized idea challenge, which uses tokenization in a blockchain to store ideas. 
The second guideline claims that the objective of design-science research is to develop technology-based solutions to important and relevant business problems. Our solution is based on a new technology and it addresses an increasing need for firms that seek to innovate.
The third guideline reminds that the utility, quality, and efficacy of a design artifact must be rigorously demonstrated via well-executed evaluation methods. We have chosen to use agent-based simulation as a form of ex-ante artificial assessment, being aware of its advantages and shortcomings.
The fourth guideline says that effective design-science research must provide clear and verifiable contributions in the areas of the design artifact, design foundations, and/or design methodologies. Our artefact builds on previous research on idea challenges and it extends it by offering design guidelines to implement it in a business context.
The fifth guidelines suggests that design-science research relies upon the application of rigorous methods in both the construction and evaluation of the design artifact. In this paper we have illustrated the kernel theories that led to our solution and the underlying assumption in our evaluation. 
The sixth guideline concerns the search for an effective artifact that requires utilizing available means to reach desired ends while satisfying laws in the problem environment. Although the simulation has used a large set of agents to test the boundary conditions of our design theory, while building our model, we took into account the tools that a firm would have to create an idea challenge. 
Finally, the seventh guideline justify our choice of targeting this conference, since design-science research must be presented effectively both to technology-oriented as well as management-oriented audiences.
This research project is currently ongoing and it has two major limitations. On the one hand, future research shall evaluate our model in a real context, by performing experimental tests with people solving the same problem while using different techniques. On the other hand, in a future paper we shall describe how to perform automatic idea clustering of the ideas by means of unsupervised algorithm that analyses the corpora of all the idea and performs text analysis.
Nonetheless, our model already offers to small and big firms the possibility to exploit a new technology to manage idea challenges in an effective and efficient way, and it opens new directions for research, such as combining game theory to assess the payoff of agents playing different theories as well as exploring the other features of blockchain to improve the performance of open innovation.

6	Conclusion
====
In this paper we have proposed a method for exploiting tokenization as a way to identify, select, and create new ideas from contests, challenges, and crowdsourcing. The current proposals present a set of limitations, whose solution will be explored in future work. In particular, the assignment of values to ideas would require further investigation. Actually, imagining that each idea receives a grade of 1 to 9 on three dimensions, and the three dimensions have different weights (100, 10 and 1), one can describe the value of the idea with numbers from 1 to 1000. The target will always be 9 * 10 ^ n is the number of dimensions, because the customer would like to have everything at its best. Furthermore, in this paper we have considered 10 ^ n + 1 to model the fact that the customer will never be satisfied. Another limitation is that the artefact is still at experimental level and the number of tested results is still too small for both justifying it and to have an explicit theory from it. As to this issue, in future work we are going to explore too different solutions that are, 1) to divide ideas into clusters (with a clustering algorithm) and take the first idea of each cluster, which allows to reduce the costs of evaluation drastically, and 2) developing and analyzing a multiple-step challenge, which can increase the payoff for agents, at limited costs for the customer. Notwithstanding the current limitations, we believe that the method would represent a first step towards a full understanding of how to exploit the opportunities offered by technologies such as blockchain for open innovation, thus bridging the gap between technical and managerial perspectives characterizing the two state of the art stances with regard the two topics. 

<!-- An example of a floating figure using the graphicx package. -->
<!-- Note that \label must occur AFTER (or within) \caption. -->
<!-- For figures, \caption should occur after the \includegraphics. -->
<!-- Note that IEEEtran v1.7 and later has special internal code that -->
<!-- is designed to preserve the operation of \label within \caption -->
<!-- even when the captionsoff option is in effect. However, because -->
<!-- of issues like this, it may be the safest practice to put all your -->
<!-- \label just after \caption rather than within \caption{}. -->

<!-- Reminder: the "draftcls" or "draftclsnofoot", not "draft", class -->
<!-- option should be used if it is desired that the figures are to be -->
<!-- displayed while in draft mode. -->

<!-- \begin{figure}[!t] -->
<!-- \centering -->
<!-- \includegraphics[width=2.5in]{myfigure} -->
<!-- where an .eps filename suffix will be assumed under latex,  -->
<!-- and a .pdf suffix will be assumed for pdflatex; or what has been declared -->
<!-- via \DeclareGraphicsExtensions. -->
<!-- \caption{Simulation results for the network.} -->
<!-- \label{fig_sim} -->
<!-- \end{figure} -->

<!-- Note that the IEEE typically puts floats only at the top, even when this -->
<!-- results in a large percentage of a column being occupied by floats. -->


<!-- An example of a double column floating figure using two subfigures. -->
<!-- (The subfig.sty package must be loaded for this to work.) -->
<!-- The subfigure \label commands are set within each subfloat command, -->
<!-- and the \label for the overall figure must come after \caption. -->
<!-- \hfil is used as a separator to get equal spacing. -->
<!-- Watch out that the combined width of all the subfigures on a  -->
<!-- line do not exceed the text width or a line break will occur. -->

<!-- \begin{figure*}[!t] -->
<!-- \centering -->
<!-- \subfloat[Case I]{\includegraphics[width=2.5in]{box}% -->
<!-- \label{fig_first_case}} -->
<!-- \hfil -->
<!-- \subfloat[Case II]{\includegraphics[width=2.5in]{box}% -->
<!-- \label{fig_second_case}} -->
<!-- \caption{Simulation results for the network.} -->
<!-- \label{fig_sim} -->
<!-- \end{figure*} -->

<!-- Note that often IEEE papers with subfigures do not employ subfigure -->
<!-- captions (using the optional argument to \subfloat[]), but instead will -->
<!-- reference/describe all of them (a), (b), etc., within the main caption. -->
<!-- Be aware that for subfig.sty to generate the (a), (b), etc., subfigure -->
<!-- labels, the optional argument to \subfloat must be present. If a -->
<!-- subcaption is not desired, just leave its contents blank, -->
<!-- e.g., \subfloat[]. -->


<!-- An example of a floating table. Note that, for IEEE style tables, the -->
<!-- \caption command should come BEFORE the table and, given that table -->
<!-- captions serve much like titles, are usually capitalized except for words -->
<!-- such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to -->
<!-- and up, which are usually not capitalized unless they are the first or -->
<!-- last word of the caption. Table text will default to \footnotesize as -->
<!-- the IEEE normally uses this smaller font for tables. -->
<!-- The \label must come after \caption as always. -->

<!-- \begin{table}[!t] -->
<!-- % increase table row spacing, adjust to taste -->
<!-- \renewcommand{\arraystretch}{1.3} -->
<!-- if using array.sty, it might be a good idea to tweak the value of -->
<!-- \extrarowheight as needed to properly center the text within the cells -->
<!-- \caption{An Example of a Table} -->
<!-- \label{table_example} -->
<!-- \centering -->
<!-- % Some packages, such as MDW tools, offer better commands for making tables -->
<!-- % than the plain LaTeX2e tabular which is used here. -->
<!-- \begin{tabular}{|c||c|} -->
<!-- \hline -->
<!-- One & Two\\ -->
<!-- \hline -->
<!-- Three & Four\\ -->
<!-- \hline -->
<!-- \end{tabular} -->
<!-- \end{table} -->


<!-- Note that the IEEE does not put floats in the very first column -->
<!-- - or typically anywhere on the first page for that matter. Also, -->
<!-- in-text middle ("here") positioning is typically not used, but it -->
<!-- is allowed and encouraged for Computer Society conferences (but -->
<!-- not Computer Society journals). Most IEEE journals/conferences use -->
<!-- top floats exclusively.  -->
<!-- Note that, LaTeX2e, unlike IEEE journals/conferences, places -->
<!-- footnotes above bottom floats. This can be corrected via the -->
<!-- \fnbelowfloat command of the stfloats package. -->


\newpage
7	References {#references .numbered}
==========
