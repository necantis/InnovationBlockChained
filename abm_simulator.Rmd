---
title: "Agent-based model simulator"
author: "Necantis"
date: "2018-08-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Theoretical model, version 01
In this initial model, we have 3 phases  
# Phase 01: Idea generation 
We assign a random number to each agent
```{r phase01.1}
target <- 1e9 # we set the target at 1'000'000'000
totAgents <- 1e3
clusters <- 10

set.seed(1) #Setting a seed to allow comparable results
ideas <- runif(totAgents, min=0, max=target) # Generating random numbers in a uniform distribution
```
We cluster the results
```{r phase01.2}
ideaClusters <- 1+as.integer(ideas/(target/clusters)) #Extracting the cluster by dividing each number to the number of clusters
agentsId <- 1:totAgents # Adding a UID
agentsReward_01 <- rep(0,totAgents) # A variable associated to phase 01
agentsReward_02 <- rep(0,totAgents) # A variable associated to phase 02
agents <- data.frame(agentsId, ideas, ideaClusters,agentsReward_01,agentsReward_02) #Creating a dataframe with ID, value and cluster

# hist(agents$ideas)
# hist(agents$ideaClusters)
```
We assign prizes
```{r phase01.3}
idClusters <- seq(1:clusters)
idWinners <- rep(NA,clusters)
valueWinners <- rep(NA,clusters)
listWinners <- data.frame(idClusters,idWinners, valueWinners) #Creating a dataframe with Cluster ID, Winner ID and value

for(i in 1:totAgents){
  j <- agents[i,3] # Check the cluster of the agent
  if(is.na(listWinners[j,2])){ # If the cluster of the agent does not have a winner (it's value is NA) ...
    listWinners[j,2] <-agents[i,1] # ... use the agent's ID
    listWinners[j,3] <-agents[i,2] # ... use the agent's value 
    agents[i,4] <- 1 # rewarding the selected agent
  }
}

listWinners
```

#Phase 02: Idea pooling 
We use [lpSOlve](https://www.kdnuggets.com/2018/05/optimization-using-r.html) to define the objective function
```{r phase02.1}
library(lpSolve)
objective.in <- listWinners[,3] # Pooling the retained ideas by looking for the best combination
```

We define the constraints of the function
```{r phase02.2}
mat <- matrix(listWinners[,3], nrow=1, byrow=TRUE) # The sum of the pooled ideas ...
dir <- "<=" # ... should be below ...
rhs <- target # ... the target
```

We solve the Linear programming function and we reward the owners of the pooled ideas
```{r phase02.3}
optimum <-lp(direction="max",  objective.in, mat, dir, rhs, all.bin = TRUE) # Which is the best combination of pooled ideas?
optimum$solution # The selected ideas

for(i in 1:clusters){
  if(optimum$solution[i] >0){ # if an idea is pooled ...
    j <- listWinners[i,2] # ... select the owner of the pooled idea
    agents[j,5] <- 1 # ... and reward the agent another time
  }
}


```
# Phase 03: Analyze results

[Filter](http://r4ds.had.co.nz/transform.html#filter-rows-with-filter) the winners
```{r phase03.1}
totWinners <- filter(agents, agentsReward_01 >0) # This could reduce computational effort
```

[Gather](http://r4ds.had.co.nz/tidy-data.html#spreading-and-gathering) the two last columns of agents into one
```{r phase03.2}
library(tidyverse)
agentsRewards <- agents %>% 
  gather('agentsReward_01', 'agentsReward_02', key = "Phase", value = "Rewards")
```
[Summarize](http://r4ds.had.co.nz/transform.html#grouped-summaries-with-summarise) by agent
```{r phase03.3}
summarise(group_by(agentsRewards, agentsId), Rewards = sum(Rewards, na.rm = TRUE))
```

Check the reminder for another idea challenge
```{r phase03.4}
remainder <- target - sum(agents$ideas*agents$agentsReward_02) #Comparing the initial target with the sum of the pooled ideas
```


## Theoretical model, version 02

Setting up a function to generate ideas
```{r}
phase00 <- function(target, totAgents,totIdeas,clusters){
  
  set.seed(1) #Setting a seed to a fix number to allow comparable results
#    agentsId <- sample(x=1:totAgents, size = totIdeas, replace = TRUE) # Assigning an owner to each idea (this allows multiple ideas to one owner)
    agentsId <- as.integer(rnorm(totIdeas, totAgents/2, totAgents/10)) # Assigning an owner to each idea (this allows multiple ideas to one owner)
    
  set.seed(1) #Setting a seed to a fix number to allow comparable results
    idea <- as.integer(runif(totIdeas, min = 0, max = target)) # Generating random numbers in a uniform distribution
  
  ideaClusters <- 1+as.integer(idea/(target/clusters)) #Extracting the cluster by dividing each number to the number of clusters
  agentsReward_01 <- rep(0,totIdeas) # A variable associated to phase 01
  agentsReward_02 <- rep(0,totIdeas) # A variable associated to phase 02
  ideas <- data.frame(agentsId, idea, ideaClusters,agentsReward_01,agentsReward_02) #Creating a dataframe with ID, value and cluster

  return(ideas)
}
```
Setting up a function to select ideas
```{r}
phase01 <- function(clusters, ideas){

  #Creating a dataframe with Cluster ID, Winner ID and value
  idClusters <- seq(1:clusters)
  idWinners <- rep(NA, clusters)
  valueWinners <- rep(NA, clusters)
  listWinners <-  data.frame(idClusters, idWinners, valueWinners) 
  
  #Reward the owners of the selected ideas
  lenIdeas <- dim(ideas)[1]

  for (i in 1:lenIdeas) {
    j <- ideas[i, 3] # Check the cluster of the idea
    if (is.na(listWinners[j, 2])) {
    # If the cluster of the agent does not have a winner (it's value is NA) ...
    listWinners[j, 2] <- ideas[i, 1] # ... use the agent's ID
    listWinners[j, 3] <- ideas[i, 2] # ... use the idea's value
    ideas[i, 4] <- 1 # rewarding the selected agent
    }
  }
  
  return(ideas)
}
```
Setting up a function to pool ideas
```{r}
phase02 <- function (selectedIdeas, target) {
  library(lpSolve)
  ideas <- selectedIdeas
  listWinners <- filter(ideas, agentsReward_01 >0)
  objective.in <- listWinners[, 2] # Pooling the retained ideas by looking for the best combination
  
  mat <-  matrix(listWinners[, 2], nrow = 1, byrow = TRUE) # The sum of the pooled ideas ...
  dir <- "<=" # ... should be below ...
  rhs <- target # ... the target
  optimum <-  lp(direction = "max", objective.in,  mat,  dir,  rhs,  all.bin = TRUE) # Which is the best combination of pooled ideas?
  optimum$solution # The selected ideas
  
  #Trying a left join
  listWinners[,5] <- optimum$solution
  polledIdeas <- ideas %>%
  left_join(listWinners, by = c("agentsId","idea","ideaClusters","agentsReward_01"))
  agentsReward_02 <- polledIdeas$agentsReward_02.x+ polledIdeas$agentsReward_02.y
  polledIdeas <- data.frame(polledIdeas,agentsReward_02)
  polledIdeas[5] <- NULL # Removing the redundant columns
  polledIdeas[5] <- NULL # Removing the redundant columns

  return(polledIdeas)
}
```
Setting up a function to summarize the rewards
```{r}
phase03 <- function(ideas){
  library(tidyverse)
  agentsRewards <- ideas %>% 
  gather('agentsReward_01', 'agentsReward_02', key = "Phase", value = "Rewards")
  return(agentsRewards)
}  
```

```{r}
ideaChallenge <- function(target, totAgents, totIdeas, clusters) {
  
  generatedIdeas <- as.data.frame(seq(1:totIdeas))
  generatedIdeas <- phase00(target, totAgents, totIdeas, clusters) # Generating ideas
  generatedIdeas <-arrange(generatedIdeas,idea) #Sorting ideas

  selectedIdeas <- phase01(clusters, generatedIdeas) # Selecting ideas

  polledIdeas <- phase02(selectedIdeas, target) # Polling ideas 
  # print(filter(polledIdeas, agentsReward_01>0)) # Test: Data check
  
  agentsRewards <- phase03(polledIdeas)
  # print(head(filter(agentsRewards,Rewards>0))) # Test: Data check
  
  return(agentsRewards)
}
```


Setting the parameters. We add a parameter totIdeas to allow an agent to have multiple ideas 
```{r}
target <- 1e9 # we set the target at 1'000'000'000
totAgents <- 1e3
totIdeas <- 1e4
  totIdeas <- max(totIdeas,totAgents) #This lines assures that there are not less ideas than agents
clusters <- 10
```
Run the simulation
```{r}

agentsRewards <- ideaChallenge(target, totAgents, totIdeas, clusters)
winners <- summarise(group_by(agentsRewards, agentsId, idea), Rewards = sum(Rewards, na.rm = TRUE))
winners <- arrange(winners,idea) #Sorting ideas
  # filter(winners, Rewards >0) # Test: selected ideas
problemSolved <- filter(winners, Rewards >1) # Test: data check
remainder <- sum(problemSolved[,2])
  
target <- target -  remainder #Setting up the remainder for a new idea challenge
target
```


#Appendix: Source of inspiration

Testing a [tutorial on ABM](https://marcosmolla.wordpress.com/2015/07/16/an-introduction-to-agent-based-modelling-in-r/)

```{r simulation1}
#Function to reset the results
setup <- function() {
  return(data.frame(
  id = 1:2,
  strategy = NA,
  num_wins = 0
  ))
}

#Function to set each agent's Strategy
chooseStrategy <- function(ind){
  strats <- sample(x = 1:3, size = nrow(ind)) # chose randomly a strategy among (1:Paper, 2:Scissors, 3:Rock) for each row in the matrix
  ind$strategy <- strats #Add a columns called strategy
  return(ind)
}

# Function to play strategy
playStrategy <- function(ind) {
  if (ind$strategy[1] == ind$strategy[2]) { # Do nothing if they have the same strategy
  } else{
  #in the case that one chose Rock and the other paper:
  if (any(ind$strategy == 3) && any(ind$strategy == 1)) {
  tmp <- ind[ind$strategy == 1, "id"]
  ind[tmp, "num_wins"] <- ind[tmp, "num_wins"] + 1
  } else{
  #for the two other cases, the better weapon wins:
  tmp <- which(ind[, "strategy"] == max(ind[, "strategy"]))
  ind[tmp, "num_wins"] <- ind[tmp, "num_wins"] + 1
  }
  }
  return(ind)
}

#Running the simulation
rounds <- 1000 # Set 1k rounds
indDF <- setup() # Initiate vector for results
dat <- matrix(NA, rounds, 2) #creates a matrix of NA, with a number of ronds equal to rounds and 2 columns (strategy and num_wins)
for (i in 1:rounds) {
  indDF <- chooseStrategy(indDF) #Give a strategy to each agent
  indDF <- playStrategy(indDF) # Agent play according to the strategy
  dat[i,] <- indDF$num_wins # Check outcomes
  i <- i + 1 # This step makes it jump by 2 each time
}

#Visualize results
plot(
  dat[, 1],
  type = 'l',
  col = '#EA2E49',
  lwd = 3,
  xlab = 'time',
  ylab = 'number of rounds won'
)
lines(dat[, 2], col = '#77C4D3', lwd = 3)

```

** Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
